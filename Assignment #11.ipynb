{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise Eleven: PCA\n",
    "Drawing on our example from class and the discussion of PCA in Data-Sitters Club,\n",
    "\n",
    "Import at least ten documents from files, using the OS module and any others relevant to process the text\n",
    "Isolate a component (the example was nouns - try verbs or adjectives) using nltk and prepare appropriate sub-files for comparison on that axis\n",
    "Load the documents and titles and run the contents through vectorize, using the provided boilerplate\n",
    "Run a simple (2 word) vizualization comparing all texts\n",
    "Run a full (PCA) vizualization comparing all texts using the provided PCA boilerplate. Note any interesting characteristics or outliers in a brief analysis\n",
    "Bonus Challenge: This shows an example of collecting the texts from PDF files. Note it is also adjusted to handle shorter texts (rather than the novels from our previous example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import os\n",
    "\n",
    "textdir = 'D:\\\\D&D\\\\Hollywood Reporter\\\\Report\\eleven\\\\pdf\\\\'\n",
    "os.chdir(textdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NLTK is the NLP package we're using\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each filename in the directory you listed...\n",
    "for filename in os.listdir(textdir):\n",
    "    #If the filename ends with .pdf...\n",
    "    if filename.endswith('.pdf'):\n",
    "        text = extract_text(filename)\n",
    "        #Create an output name that adds '-nouns' to the filename\n",
    "        outname = filename.replace('.pdf','-nouns.txt')\n",
    "        #Open the output file\n",
    "        with open(outname, 'w', errors=\"replace\") as out:\n",
    "            #Split the text into a list of sentences\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            #For each sentence in the list of sentences...\n",
    "            for sentence in sentences:\n",
    "                    #For each word and each part-of-speech tag that you get\n",
    "                    #When NLTK tokenizes the sentence (splitting words from punctuation, etc.)\n",
    "                for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):\n",
    "                    #If the part-of-speech is noun\n",
    "                    if (pos == 'NN' or pos == 'NNS'):\n",
    "                        #Write the word (which should be a noun) to the output file\n",
    "                        out.write(word)\n",
    "                        #Write a space so the words don't smush together\n",
    "                        out.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory(directory):\n",
    "    documents, titles = [], []\n",
    "    for filename in os.scandir(directory):\n",
    "        #change this to analyze a different component after changing the save above\n",
    "        if not filename.name.endswith('-nouns.txt'):\n",
    "            continue\n",
    "\n",
    "        with open(filename.path) as f:\n",
    "            contents = f.read()\n",
    "        lemmas = contents.lower().split()\n",
    "\n",
    "        documents.append(' '.join(lemmas))\n",
    "        title = filename.name.replace('-nouns.txt', '')\n",
    "        titles.append(f\"{title}\")\n",
    "    return documents, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, titles = load_directory(textdir)\n",
    "import sklearn.feature_extraction.text as text\n",
    "print(titles[0:6])\n",
    "\n",
    "vectorizer = text.CountVectorizer(max_features=30, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "v_documents = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "print(v_documents.shape)\n",
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "titles = np.array(titles)\n",
    "x = v_documents[:, words.index('audience')]\n",
    "y = v_documents[:, words.index('fan')]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for title in set(titles):\n",
    "    ax.scatter(x[titles==title], y[titles==title], label=title)\n",
    "ax.set(xlabel='audience', ylabel='fan')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "documents_proj = pca.fit_transform(v_documents)\n",
    "\n",
    "print(v_documents.shape)\n",
    "print(documents_proj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c1, c2 = documents_proj[:, 0], documents_proj[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(c1, c2, facecolors='none')\n",
    "\n",
    "for p1, p2, title in zip(c1, c2, titles):\n",
    "    ax.text(p1, p2, title, fontsize=12,\n",
    "            ha='center', va='center')\n",
    "\n",
    "ax.set(xlabel='PC1', ylabel='PC2');"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
