{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Twelve: Texts, Three Ways\n",
    "For this week, you will sample the three methods we've explored (topic modeling, sentiment analysis, and Markov chain generation) using the same set of root texts.\n",
    "\n",
    "- Collect and import ten documents (novels work best, but anything goes!)\n",
    "- Using the topic modeling code as a starter, build a topic model of the documents\n",
    "- Using the sentiment analysis code as a starter, run a sentiment analysis on sample fragments from the documents and compare\n",
    "- Using the Markov chain code as a starter, generate a sentence using one of the documents\n",
    "- Using the Markov chain code as a starter, generate a longer text fragment using all of the documents\n",
    "\n",
    "As a bonus, try to extend this analysis to note other features of these documents using any of our previous exercises as a starting point.\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One: Collect and import ten documents (novels work best, but anything goes!)\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import os\n",
    "path = \"entries/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directors = [\"Steven Spielberg\", \"George Lucas\", \"Martin Scorsese\", \"Ridley Scott\",\"M. Night Shyamalan\"]\n",
    "for director in directors:\n",
    "    page = wikipedia.page(director)\n",
    "    print(page.title)\n",
    "    filename = path + director.replace(\" \",\"_\") + \".txt\"\n",
    "    with open (filename, \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(page.content)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terry Pratchett\n",
      "Annalee Newitz\n",
      "Charlie Jane Anders\n",
      "Octavia E. Butler\n",
      "N. K. Jemisin\n"
     ]
    }
   ],
   "source": [
    "authors = [\"Terry Pratchett\", \"Annalee Newitz\", \"Charlie Jane Anders\", \"Octavia Butler\",\"N.K. Jemisin\"]\n",
    "for author in authors:\n",
    "    page = wikipedia.page(author)\n",
    "    print(page.title)\n",
    "    filename = path + author.replace(\" \",\"_\") + \".txt\"\n",
    "    with open (filename, \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(page.content)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two: Using the topic modeling code as a starter, build a topic model of the documents\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['entries/Annalee_Newitz.txt', 'entries/Charlie_Jane_Anders.txt', 'entries/N.K._Jemisin.txt', 'entries/Octavia_Butler.txt', 'entries/Terry_Pratchett.txt']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "documents = []\n",
    "path = 'entries/'\n",
    "\n",
    "filenames=sorted([os.path.join(path, fn) for fn in os.listdir(path)])\n",
    "print(len(filenames)) # count files in corpus\n",
    "print(filenames[:10]) # print names of 1st ten files in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "vectorizer=text.CountVectorizer(input='filename', stop_words=\"english\", min_df=1)\n",
    "dtm=vectorizer.fit_transform(filenames).toarray() # defines document term matrix\n",
    "\n",
    "vocab=np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix: (5, 3816). Number of tokens 10829\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of document-term matrix: {dtm.shape}. '\n",
    "      f'Number of tokens {dtm.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as decomposition\n",
    "model = decomposition.LatentDirichletAllocation(\n",
    "    n_components=100, learning_method='online', random_state=1)\n",
    "document_topic_distributions = model.fit_transform(dtm)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "# (# topics, # vocabulary)\n",
    "assert model.components_.shape == (100, len(vocabulary))\n",
    "# (# documents, # topics)\n",
    "assert document_topic_distributions.shape == (dtm.shape[0], 100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               000  0062204707  0252061400       036  0385535922  0393652673  \\\n",
      "Topic 0   0.237512    0.192002    0.193637  0.183515    0.239989    0.189205   \n",
      "Topic 1   0.182044    0.186931    0.218154  0.203321    0.200888    0.236871   \n",
      "Topic 2   0.230794    0.183528    0.187421  0.209025    0.225981    0.209132   \n",
      "Topic 3   0.193726    0.183659    0.239633  0.177718    0.213248    0.167845   \n",
      "Topic 4   0.171937    0.209294    0.169161  0.183349    0.240766    0.224978   \n",
      "...            ...         ...         ...       ...         ...         ...   \n",
      "Topic 95  0.195933    0.176922    0.186421  0.214385    0.210433    0.182803   \n",
      "Topic 96  0.177625    0.305710    0.208264  0.215818    0.261899    0.227169   \n",
      "Topic 97  0.190603    0.219492    0.205970  0.208407    0.196207    0.223520   \n",
      "Topic 98  0.183702    0.195269    0.176142  0.190611    0.172816    0.188587   \n",
      "Topic 99  0.199785    0.231579    0.176850  0.220537    0.221286    0.166682   \n",
      "\n",
      "          0431906335  0814757925  0822337454  08854300600950269  ...  \\\n",
      "Topic 0     0.233837    0.166079    0.226830           0.183024  ...   \n",
      "Topic 1     0.186400    0.167949    0.231647           0.177946  ...   \n",
      "Topic 2     0.200235    0.221765    0.192506           0.224180  ...   \n",
      "Topic 3     0.187040    0.201459    0.213645           0.201491  ...   \n",
      "Topic 4     0.178326    0.192856    0.207634           0.201153  ...   \n",
      "...              ...         ...         ...                ...  ...   \n",
      "Topic 95    0.206613    0.221756    0.194468           0.190198  ...   \n",
      "Topic 96    0.170540    0.275536    0.238556           0.213125  ...   \n",
      "Topic 97    0.192896    0.247471    0.181322           0.184464  ...   \n",
      "Topic 98    0.223678    0.182689    0.184152           0.208171  ...   \n",
      "Topic 99    0.210177    0.195762    0.191892           0.212721  ...   \n",
      "\n",
      "           youtube      ywca      zaki   zealand       zen     zenna  \\\n",
      "Topic 0   0.177140  0.190458  0.162275  0.196508  0.218816  0.197358   \n",
      "Topic 1   0.202682  0.207153  0.163207  0.201748  0.205841  0.178701   \n",
      "Topic 2   0.187801  0.206976  0.190719  0.248975  0.216720  0.194413   \n",
      "Topic 3   0.186691  0.213734  0.174666  0.174802  0.207152  0.233506   \n",
      "Topic 4   0.188800  0.212330  0.199154  0.222161  0.209977  0.204486   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "Topic 95  0.194814  0.185803  0.187734  0.210982  0.215378  0.216803   \n",
      "Topic 96  0.191777  0.223709  0.182618  0.235019  0.170706  0.210533   \n",
      "Topic 97  0.202160  0.186909  0.199582  0.200804  0.208373  0.225918   \n",
      "Topic 98  0.219017  0.220855  0.236153  0.202412  0.193031  0.231805   \n",
      "Topic 99  0.234282  0.199398  0.193789  0.193336  0.177970  0.229994   \n",
      "\n",
      "              zero    zimmer      zx81   zyzzyva  \n",
      "Topic 0   0.190420  0.238515  0.196916  0.176949  \n",
      "Topic 1   0.193347  0.245642  0.185712  0.207620  \n",
      "Topic 2   0.196221  0.230142  0.185859  0.172780  \n",
      "Topic 3   0.200700  0.189077  0.198979  0.215161  \n",
      "Topic 4   0.194246  0.211291  0.226770  0.179971  \n",
      "...            ...       ...       ...       ...  \n",
      "Topic 95  0.177894  0.218685  0.227935  0.207976  \n",
      "Topic 96  0.223543  0.194797  0.221478  0.164531  \n",
      "Topic 97  0.189552  0.195365  0.232152  0.195024  \n",
      "Topic 98  0.196155  0.229833  0.216973  0.226006  \n",
      "Topic 99  0.187011  0.219630  0.197751  0.191616  \n",
      "\n",
      "[100 rows x 3816 columns]\n"
     ]
    }
   ],
   "source": [
    "topic_names = [f'Topic {k}' for k in range(100)]\n",
    "topic_word_distributions = pd.DataFrame(\n",
    "    model.components_, columns=vocabulary, index=topic_names)\n",
    "print(topic_word_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "butler        0.289018\n",
       "page          0.285791\n",
       "links         0.285770\n",
       "lost          0.281942\n",
       "midnight      0.279050\n",
       "damaged       0.278095\n",
       "largely       0.276414\n",
       "states        0.273385\n",
       "alongside     0.269077\n",
       "hallmark      0.268642\n",
       "sensuous      0.267941\n",
       "fledgling     0.264128\n",
       "58            0.264099\n",
       "los           0.263114\n",
       "easy          0.261911\n",
       "journals      0.261848\n",
       "care          0.261702\n",
       "columnists    0.261165\n",
       "Name: Topic 9, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.loc['Topic 9'].sort_values(ascending=False).head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_876/1907171238.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "words = topic_word_distributions.loc['Topic 2'].sort_values(ascending=False).head(18)\n",
    "words\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud().generate_from_frequencies(words)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Three: Using the sentiment analysis code as a starter, run a sentiment analysis on sample fragments from the documents and compare\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\dlawb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dlawb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries/Annalee_Newitz.txt\n",
      "compound: 0.4215, neg: 0.0, neu: 0.964, pos: 0.036,  \n",
      "entries/Charlie_Jane_Anders.txt\n",
      "compound: 0.9601, neg: 0.022, neu: 0.762, pos: 0.216,  \n",
      "entries/N.K._Jemisin.txt\n",
      "compound: 0.8271, neg: 0.083, neu: 0.752, pos: 0.165,  \n",
      "entries/Octavia_Butler.txt\n",
      "compound: -0.3384, neg: 0.068, neu: 0.894, pos: 0.038,  \n",
      "entries/Terry_Pratchett.txt\n",
      "compound: 0.9062, neg: 0.0, neu: 0.851, pos: 0.149,  \n"
     ]
    }
   ],
   "source": [
    "for filename in filenames:\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        documents.append(text)\n",
    "        scores = sid.polarity_scores(text[0:500])\n",
    "    print(filename)\n",
    "    for key in sorted(scores):\n",
    "        print('{0}: {1}, '.format(key, scores[key]), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Four: Using the Markov chain code as a starter, generate a sentence using one of the documents\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import random\n",
    "generator_text = \"\"\n",
    "\n",
    "for document in documents:\n",
    "    generator_text += document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video game player, and collaborated in the creation of paper money in Ankh-Morpork.Many of the reader.\n"
     ]
    }
   ],
   "source": [
    "text_model = markovify.Text(text)\n",
    "print( text_model.make_sentence() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Five: Using the Markov chain code as a starter, generate a longer text fragment using all of the documents\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters and locations reappear throughout the series, The Long Utopia, was published by Gollancz.Pratchett gave up working for the Children's Circle section under the endless night. The video game player, and collaborated in the development of the Friends of High Wycombe Library. The final name chosen for element 117 was tennessine with the header.Pratchett's humanist funeral service was held on 25 March 2015. Pratchett was the patron of the Friends of High Wycombe Technical High School, where he was a keen astronomer from childhood. === Influences === Pratchett is known for his speech impediments. In his later years Pratchett wrote by dictating to his assistant, Rob Wilkins, wrote from the event to it. \n",
      "\n",
      "In June 2011, Pratchett presented a BBC television documentary, Terry Pratchett: Back In Black was broadcast on 11 February 2009, drawing 2.6 million viewers and a humanist. The prize is a large amount of fan mail from them. == Personal life == Pratchett married Lyn Purves at the time of his books for younger readers are divided into chapters. === Career === In 1968, Pratchett interviewed Peter Bander van Duren, co-director of a road and then destroyed by a steamroller. === Natural history === Pratchett is known for his work. === Amateur astronomy === Pratchett had mentioned two new texts, Scouting for Trolls and a humanist. He made an appearance on the topic of assisted death, but the main story. In the US, Pratchett is published by Doubleday, another Transworld imprint. \n",
      "\n",
      "In 2016, SFWA announced that he had visited as a child and donated the income from the official Terry Pratchett Memorial Scholarship === In August 2007, Pratchett announced that Sir Terry would be the recipient of the German-language version of Pyramids. Also, common spelling mistakes were used to indicate the character of Death communicating telepathically into a character's mind. A third volume, Father Christmas's Fake Beard, was released on 18 February 2009. Comic adventures that fondly mock the fantasy genre, the Discworld universe, where characters observe, and experiment on, a universe with the condition for the BBC, and became a patron for Alzheimer's Research UK. Pratchett tested a prototype device to alter the course of the Ankh-Morpork City Watch in the development of the Keyboard. \n",
      "\n",
      "The first Discworld novel, Mort. The Discworld is a wizard who was transformed into an orangutan in a much more sophisticated way than in print. The first novel, The Carpet People in 1971, which Pratchett wrote dialogue for an Oblivion mod which added a Nord companion named Vilja. The prize is a large amount of fan mail from them. All four books have chapters that alternate between fiction and attended science fiction author Stephen Baxter on a parallel earth series. He collected Brooke Bond tea cards about space, owned a greenhouse full of carnivorous plants. Pratchett was the patron of the novels in the Top 200.Pratchett received the accolade at Buckingham Palace on 18 June 2013. \n",
      "\n",
      "He has UK sales of more than 85 million books sold worldwide in 37 languages, was the top-selling and highest earning UK author in 1996. Pratchett introduced his lecture on the backs of four giant elephants, all supported by the foundation. Pratchett also included further parody as a serial in six parts, and later Equal Rites. === Career === In August 2007, Pratchett was a crime novel fan, which was reflected in frequent appearances of the 1990s. === Style and themes === Pratchett had mentioned two new texts, Scouting for Trolls and a 10.4% audience share. All four books have been published by Gollancz.Pratchett gave up working for the school magazine. A third volume, Father Christmas's Fake Beard, was released on 21 June 2012. Neil Gaiman was involved with the symbol Ts.Pratchett was a contributor to the Alzheimer's Research UK. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "novel = ''\n",
    "while (len( novel.split(\" \")) < 500):\n",
    "  for i in range(random.randrange(3,9)):\n",
    "    novel += text_model.make_sentence() + \" \"\n",
    "  novel += \"\\n\\n\"\n",
    "\n",
    "print(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Stage: Try to extend this analysis to note other features of these documents using any of our previous exercises as a starting point.\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
