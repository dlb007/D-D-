{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Twelve: Texts, Three Ways\n",
    "For this week, you will sample the three methods we've explored (topic modeling, sentiment analysis, and Markov chain generation) using the same set of root texts.\n",
    "\n",
    "- Collect and import ten documents (novels work best, but anything goes!)\n",
    "- Using the topic modeling code as a starter, build a topic model of the documents\n",
    "- Using the sentiment analysis code as a starter, run a sentiment analysis on sample fragments from the documents and compare\n",
    "- Using the Markov chain code as a starter, generate a sentence using one of the documents\n",
    "- Using the Markov chain code as a starter, generate a longer text fragment using all of the documents\n",
    "\n",
    "As a bonus, try to extend this analysis to note other features of these documents using any of our previous exercises as a starting point.\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One: Collect and import ten documents (novels work best, but anything goes!)\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import os\n",
    "path = \"entries/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directors = [\"Steven Spielberg\", \"George Lucas\", \"Martin Scorsese\", \"Ridley Scott\",\"M. Night Shyamalan\"]\n",
    "for director in directors:\n",
    "    page = wikipedia.page(director)\n",
    "    print(page.title)\n",
    "    filename = path + director.replace(\" \",\"_\") + \".txt\"\n",
    "    with open (filename, \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(page.content)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wikipedia' has no attribute 'page'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20712/1181002335.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mauthors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Terry Pratchett\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Annalee Newitz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Charlie Jane Anders\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Octavia Butler\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"N.K. Jemisin\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mauthors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mauthor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'wikipedia' has no attribute 'page'"
     ]
    }
   ],
   "source": [
    "authors = [\"Terry Pratchett\", \"Annalee Newitz\", \"Charlie Jane Anders\", \"Octavia Butler\",\"N.K. Jemisin\"]\n",
    "for author in authors:\n",
    "    page = wikipedia.page(author)\n",
    "    print(page.title)\n",
    "    filename = path + author.replace(\" \",\"_\") + \".txt\"\n",
    "    with open (filename, \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(page.content)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two: Using the topic modeling code as a starter, build a topic model of the documents\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "documents = []\n",
    "path = 'film directors/'\n",
    "\n",
    "filenames=sorted([os.path.join(path, fn) for fn in os.listdir(path)])\n",
    "print(len(filenames)) # count files in corpus\n",
    "print(filenames[:10]) # print names of 1st ten files in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "vectorizer=text.CountVectorizer(input='filename', stop_words=\"english\", min_df=1)\n",
    "dtm=vectorizer.fit_transform(filenames).toarray() # defines document term matrix\n",
    "\n",
    "vocab=np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of document-term matrix: {dtm.shape}. '\n",
    "      f'Number of tokens {dtm.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as decomposition\n",
    "model = decomposition.LatentDirichletAllocation(\n",
    "    n_components=100, learning_method='online', random_state=1)\n",
    "document_topic_distributions = model.fit_transform(dtm)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "# (# topics, # vocabulary)\n",
    "assert model.components_.shape == (100, len(vocabulary))\n",
    "# (# documents, # topics)\n",
    "assert document_topic_distributions.shape == (dtm.shape[0], 100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = [f'Topic {k}' for k in range(100)]\n",
    "topic_word_distributions = pd.DataFrame(\n",
    "    model.components_, columns=vocabulary, index=topic_names)\n",
    "print(topic_word_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_distributions.loc['Topic 9'].sort_values(ascending=False).head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = topic_word_distributions.loc['Topic 2'].sort_values(ascending=False).head(18)\n",
    "words\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud().generate_from_frequencies(words)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Three: Using the sentiment analysis code as a starter, run a sentiment analysis on sample fragments from the documents and compare\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        documents.append(text)\n",
    "        scores = sid.polarity_scores(text[0:500])\n",
    "    print(filename)\n",
    "    for key in sorted(scores):\n",
    "        print('{0}: {1}, '.format(key, scores[key]), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Four: Using the Markov chain code as a starter, generate a sentence using one of the documents\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import random\n",
    "generator_text = \"\"\n",
    "\n",
    "for document in documents:\n",
    "    generator_text += document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.Text(text)\n",
    "print( text_model.make_sentence() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Five: Using the Markov chain code as a starter, generate a longer text fragment using all of the documents\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel = ''\n",
    "while (len( novel.split(\" \")) < 500):\n",
    "  for i in range(random.randrange(3,9)):\n",
    "    novel += text_model.make_sentence() + \" \"\n",
    "  novel += \"\\n\\n\"\n",
    "\n",
    "print(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Stage: Try to extend this analysis to note other features of these documents using any of our previous exercises as a starting point.\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
