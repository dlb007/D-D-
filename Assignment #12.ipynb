{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Twelve: Texts, Three Ways\n",
    "For this week, you will sample the three methods we've explored (topic modeling, sentiment analysis, and Markov chain generation) using the same set of root texts.\n",
    "\n",
    "- Collect and import ten documents (novels work best, but anything goes!)\n",
    "- Using the topic modeling code as a starter, build a topic model of the documents\n",
    "- Using the sentiment analysis code as a starter, run a sentiment analysis on sample fragments from the documents and compare\n",
    "- Using the Markov chain code as a starter, generate a sentence using one of the documents\n",
    "- Using the Markov chain code as a starter, generate a longer text fragment using all of the documents\n",
    "\n",
    "As a bonus, try to extend this analysis to note other features of these documents using any of our previous exercises as a starting point.\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One: Collect and import ten documents (novels work best, but anything goes!)\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import os\n",
    "path = \"entries/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directors = [\"Steven Spielberg\", \"George Lucas\", \"Martin Scorsese\", \"Ridley Scott\",\"M. Night Shyamalan\"]\n",
    "for director in directors:\n",
    "    page = wikipedia.page(director)\n",
    "    print(page.title)\n",
    "    filename = path + director.replace(\" \",\"_\") + \".txt\"\n",
    "    with open (filename, \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(page.content)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terry Pratchett\n",
      "Annalee Newitz\n",
      "Charlie Jane Anders\n",
      "Octavia E. Butler\n",
      "N. K. Jemisin\n"
     ]
    }
   ],
   "source": [
    "authors = [\"Terry Pratchett\", \"Annalee Newitz\", \"Charlie Jane Anders\", \"Octavia Butler\",\"N.K. Jemisin\"]\n",
    "for author in authors:\n",
    "    page = wikipedia.page(author)\n",
    "    print(page.title)\n",
    "    filename = path + author.replace(\" \",\"_\") + \".txt\"\n",
    "    with open (filename, \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(page.content)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two: Using the topic modeling code as a starter, build a topic model of the documents\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['entries/Annalee_Newitz.txt', 'entries/Charlie_Jane_Anders.txt', 'entries/N.K._Jemisin.txt', 'entries/Octavia_Butler.txt', 'entries/Terry_Pratchett.txt']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "documents = []\n",
    "path = 'entries/'\n",
    "\n",
    "filenames=sorted([os.path.join(path, fn) for fn in os.listdir(path)])\n",
    "print(len(filenames)) # count files in corpus\n",
    "print(filenames[:10]) # print names of 1st ten files in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "vectorizer=text.CountVectorizer(input='filename', stop_words=\"english\", min_df=1)\n",
    "dtm=vectorizer.fit_transform(filenames).toarray() # defines document term matrix\n",
    "\n",
    "vocab=np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix: (5, 3816). Number of tokens 10829\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of document-term matrix: {dtm.shape}. '\n",
    "      f'Number of tokens {dtm.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as decomposition\n",
    "model = decomposition.LatentDirichletAllocation(\n",
    "    n_components=100, learning_method='online', random_state=1)\n",
    "document_topic_distributions = model.fit_transform(dtm)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "# (# topics, # vocabulary)\n",
    "assert model.components_.shape == (100, len(vocabulary))\n",
    "# (# documents, # topics)\n",
    "assert document_topic_distributions.shape == (dtm.shape[0], 100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               000  0062204707  0252061400       036  0385535922  0393652673  \\\n",
      "Topic 0   0.237512    0.192002    0.193637  0.183515    0.239989    0.189205   \n",
      "Topic 1   0.182044    0.186931    0.218154  0.203321    0.200888    0.236871   \n",
      "Topic 2   0.230794    0.183528    0.187421  0.209025    0.225981    0.209132   \n",
      "Topic 3   0.193726    0.183659    0.239633  0.177718    0.213248    0.167845   \n",
      "Topic 4   0.171937    0.209294    0.169161  0.183349    0.240766    0.224978   \n",
      "...            ...         ...         ...       ...         ...         ...   \n",
      "Topic 95  0.195933    0.176922    0.186421  0.214385    0.210433    0.182803   \n",
      "Topic 96  0.177625    0.305710    0.208264  0.215818    0.261899    0.227169   \n",
      "Topic 97  0.190603    0.219492    0.205970  0.208407    0.196207    0.223520   \n",
      "Topic 98  0.183702    0.195269    0.176142  0.190611    0.172816    0.188587   \n",
      "Topic 99  0.199785    0.231579    0.176850  0.220537    0.221286    0.166682   \n",
      "\n",
      "          0431906335  0814757925  0822337454  08854300600950269  ...  \\\n",
      "Topic 0     0.233837    0.166079    0.226830           0.183024  ...   \n",
      "Topic 1     0.186400    0.167949    0.231647           0.177946  ...   \n",
      "Topic 2     0.200235    0.221765    0.192506           0.224180  ...   \n",
      "Topic 3     0.187040    0.201459    0.213645           0.201491  ...   \n",
      "Topic 4     0.178326    0.192856    0.207634           0.201153  ...   \n",
      "...              ...         ...         ...                ...  ...   \n",
      "Topic 95    0.206613    0.221756    0.194468           0.190198  ...   \n",
      "Topic 96    0.170540    0.275536    0.238556           0.213125  ...   \n",
      "Topic 97    0.192896    0.247471    0.181322           0.184464  ...   \n",
      "Topic 98    0.223678    0.182689    0.184152           0.208171  ...   \n",
      "Topic 99    0.210177    0.195762    0.191892           0.212721  ...   \n",
      "\n",
      "           youtube      ywca      zaki   zealand       zen     zenna  \\\n",
      "Topic 0   0.177140  0.190458  0.162275  0.196508  0.218816  0.197358   \n",
      "Topic 1   0.202682  0.207153  0.163207  0.201748  0.205841  0.178701   \n",
      "Topic 2   0.187801  0.206976  0.190719  0.248975  0.216720  0.194413   \n",
      "Topic 3   0.186691  0.213734  0.174666  0.174802  0.207152  0.233506   \n",
      "Topic 4   0.188800  0.212330  0.199154  0.222161  0.209977  0.204486   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "Topic 95  0.194814  0.185803  0.187734  0.210982  0.215378  0.216803   \n",
      "Topic 96  0.191777  0.223709  0.182618  0.235019  0.170706  0.210533   \n",
      "Topic 97  0.202160  0.186909  0.199582  0.200804  0.208373  0.225918   \n",
      "Topic 98  0.219017  0.220855  0.236153  0.202412  0.193031  0.231805   \n",
      "Topic 99  0.234282  0.199398  0.193789  0.193336  0.177970  0.229994   \n",
      "\n",
      "              zero    zimmer      zx81   zyzzyva  \n",
      "Topic 0   0.190420  0.238515  0.196916  0.176949  \n",
      "Topic 1   0.193347  0.245642  0.185712  0.207620  \n",
      "Topic 2   0.196221  0.230142  0.185859  0.172780  \n",
      "Topic 3   0.200700  0.189077  0.198979  0.215161  \n",
      "Topic 4   0.194246  0.211291  0.226770  0.179971  \n",
      "...            ...       ...       ...       ...  \n",
      "Topic 95  0.177894  0.218685  0.227935  0.207976  \n",
      "Topic 96  0.223543  0.194797  0.221478  0.164531  \n",
      "Topic 97  0.189552  0.195365  0.232152  0.195024  \n",
      "Topic 98  0.196155  0.229833  0.216973  0.226006  \n",
      "Topic 99  0.187011  0.219630  0.197751  0.191616  \n",
      "\n",
      "[100 rows x 3816 columns]\n"
     ]
    }
   ],
   "source": [
    "topic_names = [f'Topic {k}' for k in range(100)]\n",
    "topic_word_distributions = pd.DataFrame(\n",
    "    model.components_, columns=vocabulary, index=topic_names)\n",
    "print(topic_word_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "butler        0.289018\n",
       "page          0.285791\n",
       "links         0.285770\n",
       "lost          0.281942\n",
       "midnight      0.279050\n",
       "damaged       0.278095\n",
       "largely       0.276414\n",
       "states        0.273385\n",
       "alongside     0.269077\n",
       "hallmark      0.268642\n",
       "sensuous      0.267941\n",
       "fledgling     0.264128\n",
       "58            0.264099\n",
       "los           0.263114\n",
       "easy          0.261911\n",
       "journals      0.261848\n",
       "care          0.261702\n",
       "columnists    0.261165\n",
       "Name: Topic 9, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.loc['Topic 9'].sort_values(ascending=False).head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31000/1907171238.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "words = topic_word_distributions.loc['Topic 2'].sort_values(ascending=False).head(18)\n",
    "words\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud().generate_from_frequencies(words)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Three: Using the sentiment analysis code as a starter, run a sentiment analysis on sample fragments from the documents and compare\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\dlawb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dlawb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries/Annalee_Newitz.txt\n",
      "compound: 0.4215, neg: 0.0, neu: 0.964, pos: 0.036,  \n",
      "entries/Charlie_Jane_Anders.txt\n",
      "compound: 0.9601, neg: 0.022, neu: 0.762, pos: 0.216,  \n",
      "entries/N.K._Jemisin.txt\n",
      "compound: 0.8271, neg: 0.083, neu: 0.752, pos: 0.165,  \n",
      "entries/Octavia_Butler.txt\n",
      "compound: -0.3384, neg: 0.068, neu: 0.894, pos: 0.038,  \n",
      "entries/Terry_Pratchett.txt\n",
      "compound: 0.9062, neg: 0.0, neu: 0.851, pos: 0.149,  \n"
     ]
    }
   ],
   "source": [
    "for filename in filenames:\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        documents.append(text)\n",
    "        scores = sid.polarity_scores(text[0:500])\n",
    "    print(filename)\n",
    "    for key in sorted(scores):\n",
    "        print('{0}: {1}, '.format(key, scores[key]), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Four: Using the Markov chain code as a starter, generate a sentence using one of the documents\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import random\n",
    "generator_text = \"\"\n",
    "\n",
    "for document in documents:\n",
    "    generator_text += document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Influences === Pratchett had an observatory built in his back garden and was a Sinclair ZX81; the first fantasy author published by Doubleday, another Transworld imprint.\n"
     ]
    }
   ],
   "source": [
    "text_model = markovify.Text(text)\n",
    "print( text_model.make_sentence() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Five: Using the Markov chain code as a starter, generate a longer text fragment using all of the documents\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sir Terry Pratchett First Novel Award === Pratchett is published by HarperCollins. In the US, Pratchett is published by Doubleday, another Transworld imprint. He collected Brooke Bond tea cards about space, owned a greenhouse full of carnivorous plants. \n",
      "\n",
      "Pratchett was misdiagnosed as having had a fascination with natural history that he wished to die by assisted suicide, his death had been working on at the local paper. The resulting social upheaval driven by these changes serves as the author continued to sign books well after the intended finishing time. === Amateur astronomy === Pratchett started to use computers for writing as soon as they were a major source of his books occupied top places on the Harry Potter newsgroup about a media-covered exchange of views with her. His 2011 Discworld novel centering on a new novel was also his first job as a feature within the Discworld universe, where characters observe, and experiment on, a universe with the symbol Ts.Pratchett was a trustee for Orangutan Foundation as their nominated charity, which has been acknowledged by the foundation. Another hallmark of his books occupied top places on the Harry Potter newsgroup about a media-covered exchange of views with her. In his later years Pratchett wrote an average of two books a year. In this position he wrote, among other things, over 80 stories for the BBC, and became a patron for Alzheimer's Research Trust, filmed a television programme chronicling his experiences with the symbol Ts.Pratchett was a Sinclair ZX81; the first Discworld novel, The Shepherd's Crown, was published in August 2015, five months after his death. \n",
      "\n",
      "Pratchett told Neil Gaiman that anything that he had been configured with the condition for the school magazine. In 2013 he gave a talk at Beaconsfield Library which he had been configured with the header.Pratchett's humanist funeral service was held on 25 March 2015. The first novel, The Colour of Magic as a child and donated the income from the official Terry Pratchett First Novel Award === Pratchett had an observatory built in his flat in a magical accident and decides to remain in that condition as it swims its way through space. \n",
      "\n",
      "The books are essentially in chronological order, and advancements can be seen in the creation of a man named Crucible who finds the Devil in his back garden and was a keen astronomer from childhood. His first computer was a keen astronomer from childhood. Neil Gaiman was involved with the author continued to sign books well after the intended finishing time. \n",
      "\n",
      "The Long Earth was released on 18 February 2009. He made an appearance on the best-seller list; he was the UK's best-selling author of the Screen. The Sir Terry would be the recipient of the German-language version of Pyramids. Pratchett had an observatory built in his back garden and was a crime novel fan, which was reflected in frequent appearances of the Discworld novels have led to dedicated conventions, the first three days. Terry Pratchett: Back In Black was broadcast on 11 February 2009, drawing 2.6 million viewers and a Discworld novel Snuff became the third-fastest-selling hardback adult-readership novel since records began in the development of the debating society and wrote stories for the BBC, and became a patron for Alzheimer's Research UK. This was followed by another collection, The Time-travelling Caveman, was released in 2017. One of Pratchett's most popular fictional characters, the Librarian, is a publishing contract with his publishers Transworld. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "novel = ''\n",
    "while (len( novel.split(\" \")) < 500):\n",
    "  for i in range(random.randrange(3,9)):\n",
    "    novel += text_model.make_sentence() + \" \"\n",
    "  novel += \"\\n\\n\"\n",
    "\n",
    "print(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Stage: Try to extend this analysis to note other features of these documents using any of our previous exercises as a starting point.\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
