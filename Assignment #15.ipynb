{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Fourteen: Project Design Starter\n",
    "In this exercise, you'll be planning out a complex project. You'll draw in some code, but focus on commenting to describe your project structure. The sample document below will guide you through organizing and annotating your project design. The primary components you'll include are:\n",
    "\n",
    "- Dependencies: What modules will your project need?\n",
    "- Collection: Where is your data coming from?\n",
    "- Processing: How will you format and process your data?\n",
    "- Analysis: What techniques will you use to understand your data?\n",
    "- Visualization: How will you visualize and explore your data?\n",
    "\n",
    "Don't worry if you aren't exactly certain how you would implement everything - this should be a starting point for a larger research study, but it doesn't need to be a complete, functional workflow. Aim for a \"good enough\" starting point that you can reference and extend for future work.\n",
    "\n",
    "Note where you have something working, and where it's broken or in progress.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview: NaNoGenMo\n",
    "This sample project builds on our previous exercises inspired by National Novel Generation Month. It offers a framework for exporing text generation based upon children's literature, inspired by NaNoGenMo's call to think about different forms of procedural making. As such, it is guided by that project's rule: \"Spend the month of November writing code that generates a novel of 50k+ words.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One: Dependencies\n",
    "\n",
    "Add the import code for every dependency of your project: for instance, if you are collecting data, you might import Tweepy or BeautifulSoup. If you're working with a file of folders, import os. Most projects will require Pandas, along with appropriate processing and visualization libraries. In the comments, explain briefly why you are including each library (as shown in the example below.)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Tracery for generative grammars\n",
    "import tracery\n",
    "# Importing Markov and dependencies\n",
    "import markovify\n",
    "import random\n",
    "# Importing OS for file sources\n",
    "import os\n",
    "# Importing nltk for word tokenization\n",
    "import nltk\n",
    "# Importing re to clean text\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two: Collection\n",
    "Describe your data collection scope and process briefly, and include an example of how you might collect your data drawing on our other projects. For example, if this workflow will collect Twitter data from a stream, you might revisit that demo, copy the stream, and adjust the hashtag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to extract our text from our novels, and we'll use regular expressions to start cleaning it of the Gutenberg header and footer \n",
    "\n",
    "up_to_word = \"Contents\"\n",
    "after_word = \"***\"\n",
    "rx_to_first = r'^.*?{}'.format(re.escape(up_to_word))\n",
    "text = \"\"\n",
    "import os\n",
    "path = 'texts/'\n",
    "with os.scandir(path) as entries:\n",
    "    temporary = \"\"\n",
    "    for entry in entries:\n",
    "        print(entry.name)\n",
    "        f = open(f'{path}\\{entry.name}',encoding='utf-8-sig')\n",
    "        temporary += f.read()\n",
    "        temporary = (re.sub(rx_to_first, '', temporary, flags=re.DOTALL).strip())\n",
    "        temporary = temporary[:temporary.index(after_word) + len(after_word)]\n",
    "        temporary = temporary.replace(\"CHAPTER \",\"\")\n",
    "        text += temporary\n",
    "print (text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Three: Processing\n",
    "After your data has been collected or imported, store it in a format that works for your purposes. This can vary: for Twitter analysis, it might be a Pandas dataframe, while for text, you might build a document term matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll pull proper nouns and verbs for our chapter headings and title\n",
    "# To avoid words that are unlikely to flow, we'll remove any word with punctuation\n",
    "def remove_punc(string):\n",
    "    string = string.lower()\n",
    "    punc = '''!()-[]{};:'’‘“-\"\\, <>—”./|?@#$%^&*_~'''\n",
    "    for ele in string:  \n",
    "        if ele in punc:  \n",
    "            return \"\"\n",
    "    return string\n",
    " \n",
    "from nltk.tag import pos_tag\n",
    "tagged_sent = pos_tag(text.split())\n",
    "propernouns = [word for word,pos in tagged_sent if pos == 'NNP']\n",
    "propernouns = [remove_punc(word) for word in propernouns]\n",
    "propernouns = list(set(propernouns))\n",
    "propernouns = [word for word in propernouns if len(word) >= 5]\n",
    "print(propernouns[0:50])\n",
    "\n",
    "verbs = [word for word,pos in tagged_sent if pos == 'VBZ']\n",
    "verbs = [remove_punc(word) for word in verbs]\n",
    "verbs = list(set(verbs))\n",
    "verbs = [word for word in verbs if len(word) >= 5]\n",
    "print(verbs[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Four: Analysis\n",
    "Think across all of the methods we've tried this semester - what combination would be most helpful for your goals? Include code sections for each method you think is important. In most cases, a combination will be most revealing: for instance, you might employ several different textual analysis frameworks on a set of documents. Use at least two distinctly different methods of analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll start with generating a chapter title function\n",
    "from tracery.modifiers import base_english\n",
    "rules = {\n",
    "    'title' : ['#noun.capitalize# #verb.capitalize# The #noun.capitalize#','#verb.capitalize# and #verb.capitalize#',\"The #noun.capitalize# and The #noun.capitalize#\",\"#directions.capitalize# #noun.capitalize#\"],\n",
    "    'noun' : propernouns,\n",
    "    'verb' : verbs,\n",
    "    'directions' : ['under','above','below','beyond','around','over','through','near','far']\n",
    "}\n",
    "grammar = tracery.Grammar(rules)\n",
    "grammar.add_modifiers(base_english)\n",
    "chapterTitle = grammar.flatten(\"#title#\")\n",
    "print(chapterTitle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an old prototype bot of mine - you'd want to write a new Tracery program\n",
    "art_rules = { \"origin\": \"<svg xmlns=\\\"http://www.w3.org/2000/svg\\\" xmlns:xlink=\\\"http://www.w3.org/1999/xlink\\\" width=\\\"1024\\\" height=\\\"512\\\">#background##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse##ellipse#</svg>\",\n",
    "\"ellipse\":\"<ellipse cx=\\\"#cx#\\\" cy=\\\"#cy#\\\" rx=\\\"#rx#\\\" ry=\\\"50\\\" style=\\\"fill:#color#;stroke:#color#;stroke-width:2;opacity:#opacity#\\\" />\",\n",
    "\"rx\":[\"50\",\"100\",\"150\",\"200\",\"250\",\"300\",\"350\"] ,\n",
    "\"color\":[\"AliceBlue\",\"Aqua\",\"Aquamarine\",\"Coral\",\"Cyan\",\"Crimson\",\"DeepPink\",\"DarkCyan\",\"DeepSkyBlue\",\"FireBrick\"\n",
    "\"Indigo\",\"SlateBlue\",\"DarkMagenta\",\"BlueViolet\",\"DarkOrchid\",\"Azure\",\"BurlyWood\",\"Bisque\",\"DarkGreen\",\"DarkSeaGreen\",\"DarkRed\",\"Fuschia\",\"IndianRed\",\"Maroon\"],\n",
    "\"opacity\":[\"0.2\",\"0.3\",\"0.4\",\"0.5\",\"0.6\",\"0.7\"],\n",
    "\"cx\":[\"100\",\"200\",\"300\",\"400\",\"500\",\"600\",\"700\",\"800\",\"900\"],\n",
    "\"cy\":[\"100\",\"200\",\"300\",\"400\",\"500\"],\n",
    "\"background\":\"<rect width=\\\"1024\\\" height=\\\"512\\\" style=\\\"fill:#backgroundcolor#\\\" />\",\n",
    "\"backgroundcolor\":[\"black\",\"gray\",\"darkgray\"]}\n",
    "art_grammar = tracery.Grammar(art_rules)\n",
    "print(art_grammar.flatten(\"#origin#\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we'll build the chapter text function\n",
    "def makeChapter(number):\n",
    "    chapterText = \"<h2>Chapter \" + str(number) + \": \"\n",
    "    chapterText += grammar.flatten(\"#title#\") + \"</h2>\"\n",
    "    chapterText += \"<p>\"\n",
    "    chapterText += \"<center>\" + art_grammar.flatten(\"#origin#\") + \"</center>\"\n",
    "    while (len(chapterText.split(\" \")) < 5000):\n",
    "      chapterText += \"<p>\"\n",
    "      for i in range(random.randrange(3,9)):\n",
    "        chapterText += text_model.make_sentence() + \" \"\n",
    "      chapterText += \"</p>\"\n",
    "    chapterText += \"</p><p></p>\"\n",
    "    return chapterText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Chapter function\n",
    "text_model = markovify.Text(text)\n",
    "print(makeChapter(1)[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Five: Visualization\n",
    "Finally, think about the visualizations that would be most useful to sharing and exploring your data. Consider both static and dynamic approaches from the different libraries we've worked with this semester. Include at least two preliminary visualizations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the novel html file\n",
    "novelTitle = grammar.flatten(\"#title#\")\n",
    "with open(\"novel.html\", \"w\", encoding=\"utf-8\", errors=\"ignore\") as f: \n",
    "    f.write(\"<html><head><title>\" + novelTitle + \"</title><style>h2 { font-family:Georgia,serif; color:#CC58DE; font-size:30px; line-height:1em; margin:0 0 0 60px; } h1 { color:#810474; font-size:85px; font-weight:normal; letter-spacing:-3.5px;line-height:1em; text-align:center;} p {font-family:'Helvetica Neue',Arial,sans-serif;font-size:15px;margin:30px 30px 30px 30px; letter-spacing:1px;} svg { max-width: 80% }</style></head><body>\")\n",
    "    f.write(\"<h1>\" + novelTitle + \"</h1>\")\n",
    "f.close()\n",
    "\n",
    "with open (\"novel.html\", \"a\") as f:\n",
    "    for chapter in range(1, 11):\n",
    "        text = \"\"\n",
    "        text = makeChapter(chapter)\n",
    "        f.write(text)\n",
    "    f.write(\"</body></html>\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
