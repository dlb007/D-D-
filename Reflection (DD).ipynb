{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Reflection (D&D)\n",
    "#### by David Bailey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this semester has been challenging but rewarding. And even though I have not mastered all aspects of the process and coding attributes, there has been growth and exploration. When considering the connections between the course coding assignments and my dissertational goals, I have discovered unexpectedly the reasoning and attributes of value by exploring iteratively, with research and experimentation of expanded use of the code, which has shifted my thoughts of code organically toward developing goals of the course thoroughly. Furthermore, considering the programming commencement at the end of the semester by the acceptance and perseverance from instructional guidance, I continued to attain comfort through reality. And looking back at the process through my access to my GitHub repositories, I have found more than technical projects; I have found a story enabling more than perseverance but openness to methods beyond what I initially thought was capable.\n",
    "\n",
    "Now, exploring the starting coding elements gave texted direction instructions and even allowed for code to present solutions. And through driven instruction and lessons, the realization of the power of code over visual components toward data expression, initially within visual chart focus to interaction, expanded with the completion of the association of image and presentation of code, making the process take on more of an organic form toward exploration and discovery. As a result, coding has connected my upcoming Remote Direction dissertational research creating a foundational discovery that has formed toward areas of interest of film and direction with the understanding of coding and relatability of visuality.\n",
    "\n",
    "As the initial challenges of coding foundations toward the comfort of working with Visual Studio Code became honest with the film directors of Wikipedia and The Hollywood Reporter, an accidental discovery from questioning if IMDb was a viable search to explore for introduction solutions toward a range of development from Coding configuration of course tools. Such as Git, Repository, Visual Studio Code, Anaconda, Python, Jupyter Notebooks, etc., toward a visual choice of technologies. Initially informative with the fundamental compilations presented value experimenting from Coding Exercise One, famed \"Hello World\" toward Coding Exercise Two that brought on imaginational aspects working the basics of Web Content. Here I randomly utilized an adaptation-focused website toward research, enabling a solid foundation to allow me to synthesize elements of web scraping and analysis. While Exercise Three in relation to Strings structured coding practice of using loops to clean up data for analysis for semblance to work on the bonus sections of rehearsed options. Spelled out a manufactured script stating, \"Got a tip? Definitive Voice of Entertainment Hollywood, Hollywood, Hollywood,\" indicating that \"The responses to different words are something you can explore as a way to engage distant reading methods\" (Salter).\n",
    "\n",
    "Moving onto Coding Exercise Four: Counting, I built on previous work scraping into an alphanumeric \"bag of words\" for processing, removing typical stop words. Using the dictionary structure in Python to assist in counting and sorting words by frequency, removing redundant word entries, and enabling comparisons toward using GitHub integrated into Visual Studio Code. I enjoyed this week's assignment, especially formulating comparisons via the \"compare_words\" directive. I was curious to see which terms would come out on top. And thankfully, I have already commenced the Pandas Jupyter Experiment (using Kaggle's Movies Dataset), indicated as a \"Nice start running comparisons and thinking about the major turning points\" (Salter).\n",
    "\n",
    "Once these areas were explored, there was more confidence and interest with the weekly upgrades commencing with Coding Exercise Five: Pandas Jupyter Experiment. I worked with a dataset available in CSV. Completing five sequential stages of importing, analyzing, and visualizing CSV data using the Pandas library, enabling every cell of Python code scripted within Markdown to allow for “Interesting choices of visualizations, the scatter matrix” (Salter). As a result, the dataset chosen in CSV was viable toward a dissertational choice of use of every cell of Python code with continued Markdown descriptions relatable to the journey of importing data relations. Here ¶Movies_metada was the .csv that I focused on as it opened up the film industry components to a data set between years and had elements to explore well-known libraries through a range of popular subjects and titles (Figure 1).\n",
    "\n",
    "Then with Stage Four, the biggest challenge for the data to be appropriately visualized and set, enabled for the goal toward three visualizations to work within any dataset. Now, the organic use of code and experimentation to unlock visuals could be used as tools, which later allowed for the Fifth assignment stage to take flight within comparisons and claims. Analyzing the data sets reveals that the data is very linearly configured on the onset, representing clarity and dynamics, eliminating components that did not communicate a focus as opportunities gained familiarity with areas through visual examples of data configuration. The work visually displayed in a Pie chart may need to be rethought. By experimenting with the color and visuals of the wheel, it showed the appeal. I chose to focus on the unnormal regions of interest with components of runtime, revenue, vote average, vote count, and taglines, mainly to see how the visual areas illustrated information randomly factored in as exportation of a Meaningful Visualization incorporated sites to demonstrate a way to secure “get_figure” from my past familiarity in the coded command (savefig), creating a workable range of filetypes, to allow the freedom of text to be expanded with minor limitations (Figure 2).\n",
    "\n",
    "Then in Coding Exercise Six, structured data projected the use of Beautiful Soup collection displayed with Pandas. And after moving from different data sources, SCMS, Museum of the Moving Image, Film Quarterly, I got pulled in with \"The Voices of Doctor Who,\" with a valued list of related film areas of interest in DHQ, of understanding of \"film discourse trends …for further clarity in the visualization\" (Salter). Each stage from importing libraries and Scrape Structured Data from Multiple Pages, even with the separation spread, the URL order, allowed for scraping value of structured data from each chosen resource. It was interesting to draw a comparison of each research perspective visually as finding associated examples, HTML files allowed for the ease of introduction and import with BeautifulSoup within the Pandas interplay, for the words from the attained coded data resource within the word_bag, to highlight the most popular discovered words visualized within their captured mode dh_counts, which enhanced the bonus attempt through iteration, visualization of storing data for external analysis filtering out five less common words on the list, toward simplistic \"Top Ten\" area focus (Figure 3).\n",
    "\n",
    "Then once the Coding Exercise Seven pertaining to textual analysis shifted toward areas that could relate through enhancement. I worked from our code examples to construct a well-documented notebook that provided a model for initial textual analysis of a multi-document corpus. I was excited to see just by chance the film focused .txt available found within Project Gutenberg's digital book repository, in examples such as \"Twelve Stories and a Dream\" of H.G. Wells to \"Night of the Living Dead\" of George A. Romero, the interest opened \"casting-related terms …in horror / genre fiction structures\" (Salter). Here I went into the referenced resource (Project Gutenberg) presented in class and explored film. The code direction allowed resources to organize alphabetically, logging in to project shortened titles from the .txt files downloaded. And with the use of OS, an observational conversation tool, I streamlined down to just seven titles from 16 to help the visualizer become vivid for navigating toward the interest of film topics found within the collection of .txt files from Behind the Screen (behind.txt), The Moving Picture Boys on the War Front (boys.txt), The Film of Fear (fear.txt), Night of the Living Dead (living.txt) to The Film Mystery (mystery.txt), Camera Actress in the Wilds of Togoland (togoland.txt), Twelve Stories and a Dream (wells.txt) (Figure 4).\n",
    "\n",
    "Once this was realized, Exercise Eight: Cultural Data and Analysis indicated a \"Nice choice of dataset! …works well for trends over time\" (Salter), where tackling cultural data enforced historical components by bringing in the relation of data on cinema. I explored the areas of cinema dataset with a focus on historical and cultural data, choosing Blockbuster.csv, which included a detailed collection of the top ten highest-grossing films from 1975 to 2018 (crowdflower dataset). Over time, the data source enhanced coding exploration using the group by methods enabled comparisons with the supplied dataset on film Genres and Ranking. While importing CSV Data, it helped push Blockbusters.csv import into a file to avoid errors toward access through separation. Pandas leveled the tone to decipher and explore trends as the data frame in the folder formed \"busters,\" as metadata formatted created value \"in a tabular format\" (Karsdorp, Kestemont, and Riddell 126) (Figure 5).\n",
    "\n",
    "As we progressed, the Coding Exercise Nine helped explore the GSS dataset introduced in the \"Social Stats\" exercise. I explored with variables 'race', 'relig', 'marital', 'agewed', 'divorce’. By importing the current version of the file and isolating the columns of interest-based on the variables chosen, \"handling the wider ranges of variables and the variety of categories of data to chart these cultural trends\" (Salter). As Stage One: Imports and Narrows by Column and Year, I imported the current version of the file (GSS7218_R1.dta) and isolated the columns of interest-based on the variables of the GSS survey data set results which are best received in the .dta over .stata (Karsdorp, Kestemont, Riddell 173) as it was essential to note that the \"Git Ignore\" option was used toward the huge .dta file to avoid any issues of stalling the process in this week's assignment. Two years were adjusted, including setting up comparable between the past 1972 and a more recent time 2016; with the use of the GSS navigator, the choice of 'agewed' was able to determine the years applicable to help narrow the dataset nicely, locating a data set 'mean' (22.1498) in age. Signifying a workable quantitative prospect that helped avoid the issue in the demo \"df = df.replace()\" with the pursuit of \"The second widely used summary statistic is the median\" (Karsdorp, Kestemont, Riddell 177).\n",
    "\n",
    "Then in Stage Two: Visualize Two Quantitative Aspects of the Data, I experimented with two opposing quantitative relationships to enable patterns to grow. First, marriage and marital age were used to help draw connections between the known numerical values, 'agewed' and marital status 'marital.' Secondly, bringing more zest to the journey and exploration was the complexity toward visualizing with \"boolean data\" value and indexing. Fostering elements to help archive opposing values, in this case, questioning toward an answer of \"yes\" and \"no\" targeted with the 'divorce' space direction (Karsdorp, Kestemont, Riddell 118-120). And for symbolic reference with alternative visual, the use of the negative component of marriage 'divorce' was used to create a pie chart counter with \"performance_counts = df['divorce'].value_counts().\" Where the discovery of \"value_counts()\" traditionally \"leaves out NaN values\" (Karsdorp, Kestemont, Riddell 27) (Figure 6).\n",
    "\n",
    "Exposing us to this week's challenge related to data collection and mapping, Coding Exercise Ten: Mapping (Twitter Style), defined coordinates, and locational elements of social media (Twitter) Tweets. I initially had challenges gaining developer approval and locating files to use within a more doable size. But, in the end, I was able to explore a \"Nice experimentation\" (Salter), where I sought out using the option for live features and attempted to use my Twitter account attaining Developer approval. Although I have not heard back from my portal application for support, I kept moving forward with the alternate TAGS and external data libraries methods, including Kaggle, as \"Scholars in the humanities and social sciences have long recognized the value of maps as a familiar and expressive medium for interpreting, explaining, and communicating scholarly work\" (Karsdorp, Kestemont, and Riddell 229). Today, most \"geographical maps using the package Cartopy, ...is Python's (emerging) standard for geospatial data processing and analysis\" (Karsdorp, Kestemont, and Riddell 230). And with the use of a \"Python library Cartopy... demonstrated how ...maps can help communicate a historical narrative\" (Karsdorp, Kestemont, and Riddell 244)—allowing us to plot a solution and move toward the bonus challenge of charting user interaction (Figure 7). \n",
    "\n",
    "With Exercise Eleven with Principle Component Analysis (PCA), I took a chance with more considerable than the usual .pdf files from the text and imagery of the digital version of a series of The Hollywood Reporter, which worked, as a \"Great use of the conversion method\" (Salter). I initially drew on the class example and the discussion of PCA in the Data-Sitters Club, which enabled the code to flow as I explored larger .pdf's in the form of The Hollywood Reporter. But also, to see if it was doable with the gigabytes even amidst imagery and photos, to allow for favoritism with the OS module to become relevant as \"Codes are both reflective and predictive. They have a past and a future\" (Benjamin 6). Here in the initial stage, I chose the use of the famed The Hollywood Reporter out of respect for the gossip and news of the film industry as well as a defined use of the material as \"Modeling depends on source data\" (Drucker 171). And the great thing about the reference is that it comes in two forms when subscribing, widespread traditional magazines and the .pdf versions. Enabling the use of Stylometry, which emphasizes the importance of modeling, to help \"answer questions about these documents\" (McCarty 2004a, 2004b) (Karsdorp, Kestemont, and Riddell 250). \n",
    "\n",
    "In the beginning, the whole file use was avoided toward extracting the text (.pdf) using pdfminer.six as gathering resources into a directory makes it easier to scan the pdf versions in use as visualization \"Problem solving is at the heart of tech\" (Benjamin 11). Here text profiles opened toward the two factors and enlightened of PCA. As intersections became visual, allowing viewership of The Hollywood Reporter's components toward content focused on within the version recognized as \"import sklearn.decomposition,\" which provides for the comfort to \"reduce the dimensionality of\" the source that is explored (Karsdorp, Kestemont, and Riddell 268). Now formulated a \"two-dimensional summary,\" reducing the focus toward a \"low-dimensional\" data set (Karsdorp, Kestemont, and Riddell 268).\n",
    "\n",
    "And with the introduction to the variance of the \"principal component PC\" (Karsdorp, Kestemont, and Riddell 268), we can use the following code to create a data set of visualization to view beyond the authorial form and clustering. Connections become more visual in this visualization as areas overlap to define even with the cluster some impact areas. The great discovery of the processes from the change of .txt focus in our previous week's samples to the chosen and changed .pdf to more possibilities. And most research that I come across is more viewed in the .pdf form and the use of The Hollywood Reporter. There was an impact in finding, even within their more extensive record, it was usable toward exploration (Figure 8).\n",
    "\n",
    "During Exercise Twelve, the Wikipedia possibilities to this week's lesson. The film and entertainment industry relies on this free online encyclopedia, and it was fun exploring the subjectable elements toward locating areas of interest. It would be interesting to find out if there are any viable code links (pip install) with IMDb (aka Internet Movie Database), recognized as \"IMDb is a little different because it doesn't have an API, but it's very well-structured for using BeautifulSoup\" (Salter). And to help push the technical philosophy further, knowing that \"An interface is a set of cognitive cues. It may look like a screen full of pictures of things inside the computer, but …mediates between an individual the computational activity\" (Drucker 176).\n",
    "\n",
    "Initially, to examine due to practicing something off the grid with a Python app toward finding a minor query, helped the path being \"director/,\" get off and running, as \"An interface can connect a person with a computer..., a computer with a computer (as in an API)\" (Drucker 172). And using the film industry's random gathering of directors listed on Wikipedia correctly to locate can at times signify a \"GLITCH... a brief, sudden interruption or irregularity\" (Benjamin 77), which some choices had to be replaced to become readable as a defined .txt entity within the process and file enabling saving page content within the director's folder to become a positive \"The notion of \"cognitive load\" (Drucker 178).\n",
    "\n",
    "Stage Two, the Topic Modeling aided the directors pulled in, was now used as a resource for a viable topic model presentation. Ranging from \"directors/Adam_Curtis.txt\" to \"directors/Mia_Hansen-Love.txt,\" this journey was accomplished as \"Feelings and stories of being surveilled are a form of evidence\" (Benjamin 80). This was an exploratory section altering extraction direction through signification of \"document_topic_distributions.loc\" ['Topic's'] (Karsdorp, Kestemont, Riddell 301), as research components \"The Matrix offers a potent allegory for thinking about power, technology, and society\" (Benjamin 84-85) come alive. Then \"Word Cloud\" generates some frequencies and sees the visually impactful visionary displays as practical strategies for finding the maximum parameters in typical mixture models with \"Expectation maximization\" that is common. As the scikit-learn library provides a convenient way of estimating the parameters, as \"following lines of code suffice to estimate all the parameters of interest\" (Karsdorp, Kestemont, Riddell 291) (Figure 9).\n",
    "\n",
    "Now, in drawing closer to the end Coding Exercise Thirteen: Interface and with this week's challenge was impactful, and the final visual use of \"BokehJS\" really set the stage for future discovery enhanced by an \"interesting choice to work with the Walking Dead game as an adaptation\" (Salter). From stage one to the end, there was a strategy toward enhancing the elements for correct visualization established as BeautifulSoup movement to work without an API, enforced with \"If your problem or question is not well defined, develop or find one which is\" (Karsdorp, Kestemont, Riddell 323). Revisiting a database and methods considering exploring other visualization types in the Bokeh API documentation, plays with the color options and scale adopts \"good enough practices in scientific computing (Wilson et al. 2017), identify a minimal set of practices which every researcher can adopt, regardless of their current level of computational skill\" (Karsdorp, Kestemont, Riddell 323). \n",
    "\n",
    "While moving on in stage with the use of exploring the URL of interest, \n",
    "https://www.metacritic.com/game/playstation-3/the-walking-dead-a-telltale-games-series/user-reviews?page=, allowed the process to \"Consider many models. Different narratives are often compatible with the same set of observations\" (Karsdorp, Kestemont, Riddell 324). The choice was timely and taken from the influence of gameplay and viewing of the show \"The Walking Dead.\" As stop words associated with limiting items were helpful as \"In the documentary film DNA Dreams, viewers are taken inside of the world's largest genomics organization, ...Thousands of scientists are working there to uncover the genetics of intelligence\" (Benjamin 114). Which enabled for visualized experimentation and expression \"creation of digital assets will then serve the project's overall design\" (Drucker 193) (Figure 10).\n",
    "\n",
    "Near the end, Coding Exercise Fourteen pertaining to Project Design allowed experimentation toward complexity. Code was handpicked due to influence during the semester, including Dependencies (Coding Pandas), Collection (The Hollywood Reporter/IMDb). Moving past the areas of development this semester indicated how coding and procedural methods would benefit from your future perspectives. Areas of engagement would help the film and media industry with media such as The Hollywood Reporter and coded examples entered of IMDb. And as I move forward, I apologize for the extensive descriptions of areas that brought interest. The movement and discovery of even the more specific areas of coding get on the curious sites of range toward the welfare of entertainment areas of coding and visualizing findings. And even though I have only touched on the beginning of these adaptations and coded interest of visual items to creativity applications, in the end, it would present an overwhelming sense of accomplishment. As moving on has now made the realization of humanizing the configuring dissertational exploration and coding toward future encoding areas that have the potential to bring in the aspects of Texts and Technology together as one. And even though the semester's challenges and guidance have enabled the process to continue further into the future of research and self-discovery.\n",
    "\n",
    "I look forward to the overall acceptance of the unknowns toward the knowns to help aspirations take on more form humanizing structures together with less effort. Thank you!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "Benjamin, Ruha. Race after Technology: Abolitionist Tools for the New Jim Code. Polity, 2019.\n",
    "\n",
    "Dombrowski, Quinn, et al. “Introduction to Jupyter Notebooks.” Programming Historian, 8 Dec. 2019, https://programminghistorian.org/en/lessons/jupyter-notebooks.\n",
    "\n",
    "Drucker, Johanna. The Digital Humanities Coursebook an Introduction to Digital Methods for Research and Scholarship. London New York Routledge, Taylor & Francis Group, 2021.\n",
    "\n",
    "Folgert Karsdorp, et al. Humanities Data Analysis Case Studies with Python. Princeton Oxford Princeton University Press, 2021.\n",
    "\n",
    "Salter, Anastasia. University of Central Florida. Texts and Technology, 2021.\n",
    "\n",
    "Turkel, William J., and Adam Crymble. “Code Reuse and Modularity in Python.” Programming Historian, 17 July 2012, https://programminghistorian.org/en/lessons/code-reuse-and-modularity.\n",
    "\n",
    "Weingart, Scott. “Doing Bayesian Data Analysis.” The Scottbot Irregular, http://www.scottbot.net/HIAL/index.html@p=8237.html.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures (Currently, in paper appendix turned in)\n",
    "Below experimentation will continue to resolve the posting challenge.\n",
    "```\n",
    "![]()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 1]\n",
    "![]()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 2]\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 3]\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 4]\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 5]\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 6]\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 7]\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 8]\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 9]\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 10]\n",
    "![]()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
